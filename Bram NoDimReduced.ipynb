{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "motivated-committee",
   "metadata": {},
   "source": [
    "## Import and Cleanup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "romantic-sending",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mca import MCA\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9e3705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded\n"
     ]
    }
   ],
   "source": [
    "## If data already preprocessed, load it in, else do preprocessing\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv('./data/Lindel_alldata.csv', index_col=0)\n",
    "    print(\"Data successfully loaded\")\n",
    "except Exception:\n",
    "    label, rev_index, features = pkl.load(open('./data/feature_index_all.pkl','rb'))\n",
    "    Lindel_training = pd.read_csv(\"./data/Lindel_training_65bp.csv\", sep=',', index_col=0)\n",
    "    Lindel_test = pd.read_csv(\"./data/Lindel_test_65bp.csv\", sep=',', index_col=0)\n",
    "\n",
    "    print(\"Number of labels : \", len(label.keys()))\n",
    "    print(\"Number of rev_index : \", len(rev_index.keys()))\n",
    "    print(\"Number of features : \", len(features.keys()))\n",
    "\n",
    "    # column descriptions\n",
    "    # Lindel_training.iloc[0] # guide sequences\n",
    "    # Lindel_training.iloc[1:3034] # 3033 binary features [2649 MH binary features + 384 one hot encoded features]\n",
    "    # Lindel_training.iloc[3034:] # 557 observed outcome frequencies\n",
    "\n",
    "    # # Merge training and test set for dimensionality reduction\n",
    "    all_data = pd.concat([Lindel_training, Lindel_test])\n",
    "    # data_features = all_data.iloc[:, 1:3034]\n",
    "\n",
    "    # # Clean up data\n",
    "    features = dict(sorted(features.items(), key=lambda item: item[1]))\n",
    "    feature_labels = list(features.keys())\n",
    "\n",
    "    labels = dict(sorted(label.items(), key=lambda item: item[1]))\n",
    "    class_labels = list(labels.keys())\n",
    "\n",
    "    one_hot_labels = []\n",
    "    for i in range(80):\n",
    "        one_hot_labels.append(\"nt {}\".format(str(int(i / 4) + 1)))\n",
    "\n",
    "    for i in range(304):\n",
    "        one_hot_labels.append(\"2nt {}\".format(str(int(i / 16) + 1)))\n",
    "\n",
    "    one_hot_labels = np.array(one_hot_labels)\n",
    "\n",
    "    column_labels = np.concatenate((np.array(['Guide Sequence', '65bp']), feature_labels, one_hot_labels, class_labels))\n",
    "\n",
    "    # Rename columns of test and training set\n",
    "    Lindel_training = Lindel_training.set_axis(column_labels, axis=1, inplace=False)\n",
    "    Lindel_test = Lindel_test.set_axis(column_labels, axis=1, inplace=False)\n",
    "\n",
    "    data = pd.concat([Lindel_training, Lindel_test], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500787a",
   "metadata": {},
   "source": [
    "## Run LINDEL Model, all configurations that do not require dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97c1bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4788, 3590) <class 'numpy.ndarray'>\n",
      "X Shape  (4788, 3033)  | y shape  (4788, 557)\n",
      "Now removing samples with only insertion events\n",
      "X_deletion Shape  (4657, 3033)  | y_deletion shape  (4657, 536)\n",
      "The first index of the first split is  466\n",
      "Number of train/val splits:  10\n"
     ]
    }
   ],
   "source": [
    "# Do data preprocessing\n",
    "\n",
    "import pickle as pkl\n",
    "import os,sys,csv,re\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.regularizers import l2, l1\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "# Make the data preprocessing a deterministic process\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define useful functions\n",
    "def mse(x, y):\n",
    "    return ((x-y)**2).mean()\n",
    "\n",
    "def corr(x, y):\n",
    "    return np.corrcoef(x, y)[0, 1] ** 2\n",
    "\n",
    "def onehotencoder(seq):\n",
    "    nt= ['A','T','C','G']\n",
    "    head = []\n",
    "    l = len(seq)\n",
    "    for k in range(l):\n",
    "        for i in range(4):\n",
    "            head.append(nt[i]+str(k))\n",
    "\n",
    "    for k in range(l-1):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                head.append(nt[i]+nt[j]+str(k))\n",
    "    head_idx = {}\n",
    "    for idx,key in enumerate(head):\n",
    "        head_idx[key] = idx\n",
    "    encode = np.zeros(len(head_idx))\n",
    "    for j in range(l):\n",
    "        encode[head_idx[seq[j]+str(j)]] =1.\n",
    "    for k in range(l-1):\n",
    "        encode[head_idx[seq[k:k+2]+str(k)]] =1.\n",
    "    return encode\n",
    "\n",
    "def kfoldsplits(X):\n",
    "    \"\"\"Split annotations\"\"\"\n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    splits = []\n",
    "    for trainIdx, validIdx in kf.split(X):\n",
    "        splits.append((trainIdx, validIdx))\n",
    "        \n",
    "    print(\"The first index of the first split is \", splits[0][0][0])\n",
    "\n",
    "    return splits\n",
    "\n",
    "# Preprocess data\n",
    "model_data = data.values[:,2:].astype(np.float32)\n",
    "print(model_data.shape, type(model_data))\n",
    "\n",
    "# Sum up deletions and insertions to\n",
    "X = model_data[:, :(2649 + 384)]\n",
    "y = model_data[:, (2649 + 384):]\n",
    "\n",
    "print(\"X Shape \", X.shape, \" | y shape \", y.shape)\n",
    "\n",
    "# Randomly shuffle data\n",
    "idx = np.arange(len(y))\n",
    "np.random.shuffle(idx)\n",
    "X, y = X[idx], y[idx]\n",
    "\n",
    "print(\"Now removing samples with only insertion events\")\n",
    "X_deletion, y_deletion = [], []\n",
    "\n",
    "# Remove samples that only have insertion events:\n",
    "for i in range(model_data.shape[0]):\n",
    "    if 1> sum(y[i,:536])> 0 :\n",
    "        y_deletion.append(y[i,:536]/sum(y[i,:536]))\n",
    "        X_deletion.append(X[i])\n",
    "        \n",
    "X_deletion, y_deletion = np.array(X_deletion), np.array(y_deletion)\n",
    "\n",
    "print(\"X_deletion Shape \", X_deletion.shape, \" | y_deletion shape \", y_deletion.shape)\n",
    "\n",
    "\n",
    "\n",
    "splits = kfoldsplits(X_deletion)\n",
    "print(\"Number of train/val splits: \", len(splits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434386e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save dir is  ./results/20-03-2022 20:54:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a3720d7ca3bf>:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range(len(splits))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a997bd33c01c489c9792398ffe8778cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline  1 of 10\n",
      "Baseline  2 of 10\n",
      "Baseline  3 of 10\n",
      "Baseline  4 of 10\n"
     ]
    }
   ],
   "source": [
    "# Make results dir\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "save_dir = os.path.join('./results', dt_string)\n",
    "os.makedirs(save_dir)\n",
    "\n",
    "print(\"Save dir is \", save_dir)\n",
    "\n",
    "# Train model: No regularization, no early stopping\n",
    "for i in tqdm(range(len(splits))):\n",
    "    print(\"Baseline \", (i+1), \"of 10\")\n",
    "\n",
    "    train_split, val_split = splits[i]\n",
    "    \n",
    "    x_train = X_deletion[train_split]\n",
    "    x_valid = X_deletion[val_split]\n",
    "    \n",
    "    y_train = y_deletion[train_split]\n",
    "    y_valid = y_deletion[val_split]\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"{}/logs/baseline\".format(save_dir))\n",
    "    \n",
    "    checkpoint_name = save_dir + '/baseline_cp{}'.format(i)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath= checkpoint_name + '-{epoch:02d}.h5',\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode=\"min\")\n",
    "    \n",
    "    csv_logger = CSVLogger('{}/baseline{}.log'.format(save_dir, i), separator=',', append=False)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(536,  activation='softmax', input_shape=(X_deletion.shape[1],), kernel_regularizer=None))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['mse'])\n",
    "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_valid, y_valid),\n",
    "              callbacks=[\n",
    "                  tensorboard_callback,\n",
    "                  model_checkpoint_callback,\n",
    "                  csv_logger], verbose=0)\n",
    "\n",
    "    \n",
    "# Train model: L1 and L2, no early stopping\n",
    "baseline_errors = []\n",
    "for i in tqdm(range(len(splits))):\n",
    "    print(\"L1 / L2 \", (i+1), \"of 10\")\n",
    "\n",
    "    train_split, val_split = splits[i]\n",
    "    \n",
    "    x_train = X_deletion[train_split]\n",
    "    x_valid = X_deletion[val_split]\n",
    "    \n",
    "    y_train = y_deletion[train_split]\n",
    "    y_valid = y_deletion[val_split]\n",
    "    \n",
    "    # L1 Regularization\n",
    "    lambdas = 10 ** np.arange(-10, -1, 0.1)\n",
    "\n",
    "    for l in tqdm(lambdas):\n",
    "        tensorboard_callback_l1 = tf.keras.callbacks.TensorBoard(log_dir=\"{}/logs/l1\".format(save_dir))\n",
    "    \n",
    "        checkpoint_name = save_dir + '/l1_{}_lambda_{}'.format(i, l)\n",
    "\n",
    "        csv_logger = CSVLogger('{}/L1_{}.log'.format(save_dir, i), separator=',', append=False)\n",
    "        \n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(536,  activation='softmax', input_shape=(X_deletion.shape[1],), kernel_regularizer=l1(l)))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['mse'])\n",
    "        history = model.fit(x_train, y_train, epochs=100, validation_data=(x_valid, y_valid), verbose=0,\n",
    "                           callbacks=[tensorboard_callback, csv_logger,\n",
    "                                tf.keras.callbacks.ModelCheckpoint(\n",
    "                                    filepath= checkpoint_name + '-{epoch:02d}.h5',\n",
    "                                    monitor=\"val_loss\",\n",
    "                                    verbose=0,\n",
    "                                    save_best_only=True,\n",
    "                                    save_weights_only=False,\n",
    "                                    mode=\"min\"\n",
    "                                )\n",
    "                           ])\n",
    "        \n",
    "    # L2 Regularization\n",
    "    for l in tqdm(lambdas):\n",
    "        tensorboard_callback_l2 = tf.keras.callbacks.TensorBoard(log_dir=\"{}/logs/l2\".format(save_dir))\n",
    "    \n",
    "        checkpoint_name = save_dir + '/l2_{}_lambda_{}'.format(i, l)\n",
    "\n",
    "        csv_logger = CSVLogger('{}/L2_{}.log'.format(save_dir, i), separator=',', append=False)\n",
    "        \n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(536,  activation='softmax', input_shape=(X_deletion.shape[1],), kernel_regularizer=l2(l)))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['mse'])\n",
    "        history = model.fit(x_train, y_train, epochs=100, validation_data=(x_valid, y_valid), verbose=0,\n",
    "                           callbacks=[tensorboard_callback, csv_logger,\n",
    "                                tf.keras.callbacks.ModelCheckpoint(\n",
    "                                    filepath= checkpoint_name + '-{epoch:02d}.h5',\n",
    "                                    monitor=\"val_loss\",\n",
    "                                    verbose=0,\n",
    "                                    save_best_only=True,\n",
    "                                    save_weights_only=False,\n",
    "                                    mode=\"min\"\n",
    "                                )\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b34dc2307e58e1a1d1e57cf7ef01b077b034c069b8a201720687e01a8602b64b"
  },
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "bio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}